{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbf2de8",
   "metadata": {},
   "source": [
    "# NDT Global - Technical Assessment Machine Learning Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721c3b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0d6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import layers, models\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c396f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available (Apple's Metal backend)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1ed2a",
   "metadata": {},
   "source": [
    "Define a custom dataloader optimsed for data augmentation with transforamtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659350cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# CustomImageDataset() :: Custom dataset class to load images    #\n",
    "##################################################################\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        # Assume folder structure is img_dir/class_x/img1.jpg\n",
    "        for label, class_dir in enumerate(os.listdir(img_dir)):\n",
    "            class_folder = os.path.join(img_dir, class_dir)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for img_file in os.listdir(class_folder):\n",
    "                    self.image_paths.append(os.path.join(class_folder, img_file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c63cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# split_data() ::  Function to split data into training, validation and test sets        #\n",
    "##########################################################################################\n",
    "def split_data(dataset, test_split=0.1, val_split=0.1, shuffle=True, seed=37):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    test_size = int(np.floor(test_split * dataset_size))\n",
    "    val_size = int(np.floor(val_split * (dataset_size - test_size)))  # Validation based on remaining data\n",
    "    \n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    test_indices = indices[:test_size]\n",
    "    remaining_indices = indices[test_size:]\n",
    "    \n",
    "    val_indices = remaining_indices[:val_size]\n",
    "    train_indices = remaining_indices[val_size:]\n",
    "    \n",
    "    return train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa431208",
   "metadata": {},
   "source": [
    "Function to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#  train_model() :: Training function                                                                               #\n",
    "#      patience   :: Training will stop if no significant improvement takes place in \"patience\" epochs.             #\n",
    "#      threshold  :: Relative improvement required for validation acc or loss to reset the patience counter.        #\n",
    "#####################################################################################################################\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, scheduler, patience=35, threshold=0.02, save_path=\"./Model/Simple-CNN/\", save_name =\"model.pth\"):\n",
    "    \n",
    "    # Keep track of training and validation loss\n",
    "    valid_accuracy_max = 0                       # Itinizalize max validation accuracy\n",
    "    valid_loss_min = np.Inf                      # Initialize minimum validation loss\n",
    "    train_losses, valid_losses = [], []          # Store loss values for plotting\n",
    "    train_accuracies, valid_accuracies = [], []  # Store accuracy values for plotting\n",
    "    no_improvement_epochs = 0                    # Initialize patience counter\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        ###################\n",
    "        # train the model # --> train_loader\n",
    "        ###################\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0        \n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()               # Clear the gradients\n",
    "            output = model(images)              # Forward pass, calculate ouput\n",
    "            loss = criterion(output, labels)    # Calculate the loss\n",
    "            loss.backward()                     # Backpropagation\n",
    "            optimizer.step()                    # Update weights\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)  # Track training loss\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)  # Get predicted class\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        ######################\n",
    "        # validate the model # --> valid_loader\n",
    "        ######################\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for images, labels in valid_loader:\n",
    "                if train_on_gpu:\n",
    "                    images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "                output = model(images)                      # Forward pass\n",
    "                loss = criterion(output, labels)            # Calculate the validation loss\n",
    "                \n",
    "                valid_loss += loss.item() * images.size(0)   # Track validation loss\n",
    "\n",
    "                _, predicted = torch.max(output, 1)         # Get predicted class\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                total_valid += labels.size(0)\n",
    "        \n",
    "        ###################\n",
    "        #  Derive metrics #\n",
    "        ###################\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "        #train_loss = train_loss / len(train_loader)  \n",
    "        #valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Calculate training and validation accuracy\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        valid_accuracy = 100 * correct_valid / total_valid\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        # Print training/validation statistics\n",
    "        print(f'\\nEpoch: {epoch} \\n\\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {valid_loss:.6f} \\n\\tTraining Accuracy: {train_accuracy:.2f}% \\tValidation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n",
    "        #####\n",
    "        # Save the model if validation loss has decreased\n",
    "        ######\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        save_full_path = os.path.join(save_path, save_name)\n",
    "        \n",
    "        if valid_loss < valid_loss_min:  \n",
    "            print(f'Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model ...')\n",
    "            torch.save(model.state_dict(), save_full_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            #valid_accuracy_max = valid_accuracy\n",
    "            \n",
    "            \n",
    "        ####\n",
    "        # Early stop\n",
    "        ### \n",
    "        relative_loss_improvement = (valid_loss_min - valid_loss) / valid_loss_min if valid_loss_min != 0 else 0\n",
    "        relative_accuracy_improvement = (valid_accuracy - valid_accuracy_max) / valid_accuracy_max if valid_accuracy_max != 0 else 0\n",
    "\n",
    "\n",
    "        # Check if loss and accuracy improved by more than the threshold\n",
    "        if (relative_loss_improvement > threshold) or (relative_accuracy_improvement > threshold):\n",
    "            valid_accuracy_max = valid_accuracy\n",
    "            no_improvement_epochs = 0  # Reset patience counter \n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f'Early stopping: No improvement in validation for {patience} epochs.')\n",
    "            break  \n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21341bc6",
   "metadata": {},
   "source": [
    "Function to create model\n",
    "Adjust for number of clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cec728",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#  My_CNN() ::                              #\n",
    "###############################################\n",
    "class My_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_CNN, self).__init__()\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)  # 3x3 kernel, padding to preserve size\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization after conv1\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)  # Increase number of filters\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Fourth Convolutional Layer (Optional)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Global Average Pooling (instead of Flatten)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # Creates a 1x1 image with many channels\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256, 128)  # Match input to output channels of the last conv layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Final output for 10 classes\n",
    "\n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional Block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)  # Reduce size by half\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Fourth Convolutional Block (Optional)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.global_avg_pool(x)  # Reduces to (batch_size, 256, 1, 1) # 256 = number of channels\n",
    "        x = x.view(x.size(0), -1)    # Flatten to (batch_size, 256)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout for regularization\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = My_CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a089d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# plot_images_from_loader() :: Used to test the impact of transformations \n",
    "############################################################################\n",
    "def plot_images_from_loader(loader):\n",
    "    # Get a batch of images and labels\n",
    "    images, labels = next(iter(loader))\n",
    "    \n",
    "    # Set up a 2x3 subplot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n",
    "    \n",
    "    # Loop through the first 6 images\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Convert tensor image to numpy array and adjust for matplotlib\n",
    "        img = images[i].numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Plot the image\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Label: {labels[i].item()}')\n",
    "        ax.axis('off')  # Hide axis\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "########################################################\n",
    "# plot_accuracy() :: training and validation accuracy  #\n",
    "########################################################\n",
    "def plot_accuracy(train_accuracies, valid_accuracies):\n",
    "    epochs = range(1, len(train_accuracies) + 1)  # defined like this for the event of early stopping\n",
    "    plt.plot(epochs, train_accuracies, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, valid_accuracies, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "################################################\n",
    "# plot_loss() :: training and validation loss  #\n",
    "################################################\n",
    "def plot_loss(train_losses, valid_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)  \n",
    "    plt.plot(epochs, train_losses, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, valid_losses, 'b', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d73c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# test_model() :: Evaluates the performance of the model on the given test_loader  #\n",
    "####################################################################################\n",
    "def test_model(model, test_loader, criterion):\n",
    "    # Load the weights of the model to test\n",
    "    #model.load_state_dict(torch.load('./Model/Simple-CNN/model.pth'))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    # Turn off gradients for test evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass: compute predictions\n",
    "            output = model(images)\n",
    "            \n",
    "            # Calculate the test loss\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item() * images.size(0)  # Accumulate the test loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output, 1)  # Get predicted class\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "    \n",
    "    # Calculate average test loss\n",
    "    test_loss = test_loss / len(test_loader.sampler)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    \n",
    "    print(f'\\nTest Loss: {test_loss:.6f} \\nTest Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e877ca7",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed72e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#   Load the data  #\n",
    "####################\n",
    "\n",
    "\n",
    "# Data path\n",
    "data_dir = 'path_to_train_data'\n",
    "\n",
    "\n",
    "# Transformations\n",
    "# Define transformations for train, validation and test datasets\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# No augmentations for validation and test datasets\n",
    "no_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = CustomImageDataset(data_dir, transform=train_transform)\n",
    "val_dataset = CustomImageDataset(data_dir, transform=no_transform)\n",
    "test_dataset = CustomImageDataset(data_dir, transform=no_transform)\n",
    "\n",
    "\n",
    "# Split data with indices\n",
    "train_indices, val_indices, test_indices = split_data(train_dataset, test_split=0.1, val_split=0.1)\n",
    "\n",
    "# Create data samplers and loaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "\n",
    "batch_size = 32 #reduce if memory issues. Try out batch sizes of 16, 32, and 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "# Confirm the splits\n",
    "print(f\"Number of training samples: {len(train_indices)}\")\n",
    "print(f\"Number of validation samples: {len(val_indices)}\")\n",
    "print(f\"Number of test samples: {len(test_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "plot_images_from_loader(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a83e6",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Train hyperparameters\n",
    "#########################\n",
    "\n",
    "# Specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify optimizer. The most popular optimizers are Adam and SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)            # Stochastic Gradient Descent\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Adaptive Moment Estimation\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 40\n",
    "\n",
    "\n",
    "train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(\n",
    "    model, train_loader, val_loader,  \n",
    "    criterion, optimizer, num_epochs=n_epochs, \n",
    "    scheduler=scheduler, patience=100, threshold=0.01, \n",
    "    save_path=\"./Model/Test_CNN/\", save_name=\"model_01.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298aa43",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(train_accuracies, valid_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
